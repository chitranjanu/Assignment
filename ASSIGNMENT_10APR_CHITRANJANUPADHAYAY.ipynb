{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer1.\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the insurance plan, we can use Bayes' theorem.\n",
    "\n",
    "Let's define the events:\n",
    "A: Employee is a smoker.\n",
    "B: Employee uses the company's health insurance plan.\n",
    "\n",
    "We are given the following probabilities:\n",
    "P(B) = 0.7 (probability that an employee uses the insurance plan)\n",
    "P(A|B) = 0.4 (probability that an employee is a smoker given that they use the insurance plan)\n",
    "\n",
    "We want to find P(A|B), the probability that an employee is a smoker given that they use the insurance plan.\n",
    "\n",
    "Using Bayes' theorem:\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "In this case, P(B|A) is the probability that an employee uses the insurance plan given that they are a smoker. However, we are not given this probability directly. To proceed, we need to make an additional assumption or have additional information.\n",
    "\n",
    "If we assume that the probability of an employee using the insurance plan is independent of whether they are a smoker or not, then P(B|A) is equal to P(B), and we can proceed with the calculation:\n",
    "\n",
    "P(A|B) = (P(B) * P(A)) / P(B)\n",
    "= (0.7 * 0.4) / 0.7\n",
    "= 0.4\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that they use the insurance plan is 0.4 or 40%.\n",
    "\n",
    "However, it's important to note that this result is based on the assumption of independence between using the insurance plan and being a smoker. If this assumption does not hold, the calculation would require additional information or a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer2.\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in their assumptions about the distribution of features and the representation of the data.\n",
    "\n",
    "Feature Distribution:\n",
    "Bernoulli Naive Bayes: It assumes that the features are binary or boolean in nature, meaning they take on values of either 0 or 1. It models the presence or absence of features in the data. It assumes a series of independent Bernoulli trials for each feature, hence the name \"Bernoulli.\" The feature distribution is binomial, with each feature being treated as a binary random variable.\n",
    "Multinomial Naive Bayes: It assumes that the features are discrete and follow a multinomial distribution. It is commonly used in text classification tasks where features represent word frequencies or occurrence counts. It models the counts or frequencies of different features in the data.\n",
    "Data Representation:\n",
    "Bernoulli Naive Bayes: It is suitable for binary feature representations where the focus is on whether a feature is present or absent. It often represents the data in the form of binary vectors or matrices, indicating the presence (1) or absence (0) of features.\n",
    "Multinomial Naive Bayes: It is suitable for multi-valued discrete feature representations, typically representing the data as frequency vectors or matrices, where each entry denotes the count or frequency of a specific feature.\n",
    "Feature Independence:\n",
    "Both Bernoulli Naive Bayes and Multinomial Naive Bayes assume feature independence given the class label, which is the core assumption of Naive Bayes classifiers. However, the interpretation of feature independence may vary slightly based on the feature distribution and representation used.\n",
    "In summary, Bernoulli Naive Bayes is appropriate when dealing with binary features and focusing on feature presence or absence. It models the data using a binomial distribution. On the other hand, Multinomial Naive Bayes is suitable for discrete features represented by counts or frequencies and assumes a multinomial distribution. It is commonly used in text classification tasks. The choice between the two depends on the nature of the features and the data representation that aligns with the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50226beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer3.\n",
    "\n",
    "In Bernoulli Naive Bayes, missing values are typically handled by considering them as a separate category or treating them as a distinct value for the corresponding feature. Here are a couple of common approaches to dealing with missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "Separate category approach: In this approach, missing values are treated as a separate category or state for the feature. Instead of assigning a binary value (0 or 1) to represent the presence or absence of a feature, a third category (e.g., \"unknown\" or \"missing\") is introduced to indicate that the value is missing. During training, the model estimates the probabilities for the \"unknown\" category alongside the binary categories. Then, during classification, if a feature value is missing, it is considered as the \"unknown\" category and incorporated into the probability calculations accordingly.\n",
    "\n",
    "Data imputation approach: Another approach is to impute or fill in the missing values based on some imputation technique before training the Bernoulli Naive Bayes model. There are several methods for imputing missing values, such as replacing missing values with the mode (most frequent value) of that feature or using more sophisticated imputation techniques like regression or nearest neighbors. Once the missing values are imputed, the Bernoulli Naive Bayes model can be trained using the complete dataset.\n",
    "\n",
    "It's important to note that the choice of the approach depends on the specific characteristics of the dataset and the nature of the missing values. Both approaches have their own trade-offs, and the most suitable approach should be determined based on the nature of the data and the problem at hand.\n",
    "\n",
    "Additionally, it is crucial to consider the potential impact of missing values on the model's performance and the validity of the assumptions made by Naive Bayes, as the independence assumption may not hold when missing values are informative or systematically related to other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer4.\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of Naive Bayes that assumes a Gaussian (normal) distribution for continuous features. While the original Naive Bayes algorithm is commonly used for binary or multi-class classification, Gaussian Naive Bayes specifically handles continuous features.\n",
    "\n",
    "For multi-class classification using Gaussian Naive Bayes, the algorithm estimates the mean and variance of each feature for each class. During training, it calculates the class-specific mean and variance for each feature based on the training data. These estimates are then used to model the Gaussian distribution for each feature and each class.\n",
    "\n",
    "To classify a new instance, Gaussian Naive Bayes applies Bayes' theorem and calculates the posterior probability for each class given the features. It does this by assuming that the likelihood of each feature belonging to a class follows a Gaussian distribution with the class-specific mean and variance. The class with the highest posterior probability is predicted as the output class.\n",
    "\n",
    "So, in summary, Gaussian Naive Bayes can indeed be used for multi-class classification by modeling the class-specific Gaussian distributions for each continuous feature and calculating the posterior probabilities based on those distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer5.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Fetch the Spambase dataset from UCI repository\n",
    "dataset = fetch_openml(name='spambase', version=1)\n",
    "\n",
    "# Split features and labels\n",
    "X = dataset.data\n",
    "y = dataset.target.astype('int')\n",
    "\n",
    "# Define the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate performance metrics for each classifier\n",
    "classifiers = {'Bernoulli Naive Bayes': bernoulli_nb, 'Multinomial Naive Bayes': multinomial_nb, 'Gaussian Naive Bayes': gaussian_nb}\n",
    "\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(classifier, X, y, cv=10)\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print(classifier_name)\n",
    "    print('Accuracy:', np.mean(scores))\n",
    "    print('Precision:', np.mean(cross_val_score(classifier, X, y, cv=10, scoring='precision')))\n",
    "    print('Recall:', np.mean(cross_val_score(classifier, X, y, cv=10, scoring='recall')))\n",
    "    print('F1 Score:', np.mean(cross_val_score(classifier, X, y, cv=10, scoring='f1')))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
