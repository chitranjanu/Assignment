{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a923ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer1.\n",
    "\n",
    "Polynomial kernel functions are a specific type of kernel function that captures the similarity between data points based on the polynomial expansion of their feature vectors. The polynomial kernel function calculates the inner product between the transformed feature vectors in a higher-dimensional space without explicitly performing the transformation.\n",
    "\n",
    "The polynomial kernel function is defined as:\n",
    "\n",
    "K(x, y) = (α * (x • y) + c)^d\n",
    "\n",
    "where x and y are the input feature vectors, α is a user-defined constant, c is an optional constant, and d is the degree of the polynomial.\n",
    "\n",
    "The polynomial kernel function implicitly represents the nonlinear mappings induced by polynomial functions. It allows SVMs to effectively model and classify nonlinear relationships by operating in a higher-dimensional space.\n",
    "\n",
    "In summary, polynomial functions are mathematical functions used to define nonlinear mappings of input features, while polynomial kernel functions are a specific type of kernel function that captures the similarity between data points based on the polynomial expansion of their feature vectors. Polynomial kernel functions enable SVMs to handle nonlinear classification problems by implicitly operating in a higher-dimensional space defined by polynomial functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer2.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "svm_classifier = SVC(kernel='poly', degree=3)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3)\n",
    "\n",
    "# Training the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluating the performance of the SVM classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cfeb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer3.\n",
    "\n",
    "In Support Vector Regression (SVR), increasing the value of epsilon does not directly affect the number of support vectors. The number of support vectors in SVR is primarily determined by the complexity of the data distribution and the chosen kernel function.\n",
    "\n",
    "Support vectors are the data points that have non-zero coefficients in the SVR model, meaning they contribute to defining the regression function and determining the position of the regression hyperplane.\n",
    "\n",
    "Epsilon (ε) in SVR represents the size of the epsilon-insensitive tube, which determines the margin around the regression line where errors are tolerated. Any data points falling within this tube are considered support vectors. The value of epsilon controls the width of this tube and affects the tolerance for errors or deviations from the regression line.\n",
    "\n",
    "However, the number of support vectors is more influenced by the complexity of the data and the kernel function used rather than the value of epsilon. Increasing epsilon may allow for larger deviations from the regression line to be considered acceptable, potentially reducing the number of support vectors. Conversely, decreasing epsilon may result in a narrower tube, leading to more support vectors.\n",
    "\n",
    "In summary, while the value of epsilon in SVR affects the margin and the tolerance for errors, it does not have a direct and deterministic effect on the number of support vectors. The number of support vectors is primarily influenced by the complexity of the data and the chosen kernel function, which determine the need for capturing the details of the data distribution and the shape of the regression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21364efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer4.\n",
    "\n",
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter in Support Vector Regression (SVR) can have a significant impact on the performance and behavior of the SVR model. Here's how each of these parameters affects the SVR performance:\n",
    "\n",
    "Kernel function:\n",
    "The kernel function determines the type of mapping used to transform the input features into a higher-dimensional space. The choice of kernel function depends on the nature of the data and the underlying relationship. Different kernel functions, such as linear, polynomial, radial basis function (RBF), or sigmoid, capture different types of relationships between the input features and the target variable. The appropriate kernel function should be chosen based on the characteristics of the data and the problem at hand.\n",
    "\n",
    "C parameter:\n",
    "The C parameter in SVR controls the trade-off between the complexity of the model and the degree of errors tolerated. It represents the regularization parameter that balances the margin width and the errors allowed within that margin. A smaller C value allows for a wider margin but tolerates more errors, potentially leading to underfitting. On the other hand, a larger C value results in a narrower margin, enforcing a tighter fit to the training data and potentially increasing the risk of overfitting. The choice of the C parameter should be made through cross-validation, considering the bias-variance trade-off and the generalization performance of the model.\n",
    "\n",
    "Epsilon parameter:\n",
    "The epsilon (ε) parameter in SVR determines the size of the epsilon-insensitive tube. It represents the tolerance for errors or deviations from the regression line within the tube. Increasing epsilon allows for larger deviations to be considered acceptable, potentially increasing the number of support vectors. Conversely, decreasing epsilon results in a narrower tube, requiring a closer fit to the data and potentially reducing the number of support vectors. The choice of epsilon depends on the acceptable level of error or deviation in the regression predictions.\n",
    "\n",
    "Gamma parameter:\n",
    "The gamma parameter is specific to kernel functions such as RBF and sigmoid. It defines the width of the kernel and controls the influence of individual training samples. A smaller gamma value leads to a broader kernel, considering more samples as support vectors and potentially resulting in a smoother decision boundary. Conversely, a larger gamma value narrows the kernel, giving more weight to nearby samples and potentially resulting in a more complex and localized decision boundary. The choice of gamma depends on the data distribution and the desired smoothness or complexity of the regression model.\n",
    "\n",
    "It's important to note that the optimal values for these parameters depend on the specific dataset and the problem at hand. It is recommended to perform hyperparameter tuning and model selection techniques, such as cross-validation or grid search, to find the best combination of kernel function, C, epsilon, and gamma for achieving optimal SVR performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer5.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of SVC classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using accuracy metric\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
